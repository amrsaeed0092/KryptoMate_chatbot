{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JQn0QUeG7eD"
      },
      "source": [
        "\n",
        "### Crawl a website and store HTML to a Google Cloud Bucket\n",
        "\n",
        "This colab gives you the ability to crawl your website, and put the HTML files into a Google Cloud Bucket you control.\n",
        "\n",
        "**What is \"Google Cloud Bucket\"?**\n",
        "\n",
        "A Google Cloud Bucket is a container for storing any type of data in Google [Cloud Storage](https://storage.cloud.google.com/).\n",
        "\n",
        "**What is \"Scrapy\"?**\n",
        "\n",
        "[Scrapy](http://scrapy.org) is a free and open-source web scraping framework that can be used to crawl websites and extract data. It can be customized in many ways, both in what URLs you allow to be crawled and also in what outputs you want to save.\n",
        "\n",
        "**Why?**\n",
        "\n",
        "Perhaps you need to consume unstructured HTML data from a bucket, like for [Vertex AI Search and Conversation](https://cloud.google.com/vertex-ai-search-and-conversation).  _NOTE: The Vertex AI Search and Conversation Website Crawler is significantly better, this is a DIY example._\n",
        "\n",
        "<small><em>blame: alanblount@google.com</em></small>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install twisted\n",
        "from twisted.internet import asyncioreactor\n",
        "asyncioreactor.install()"
      ],
      "metadata": {
        "id": "6nsp3uUUHOG5"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x47vqAKxG-ti"
      },
      "source": [
        "### Alternatives"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LC0dAWK-2unE"
      },
      "source": [
        "Here are some other very easy to use apps which can crawl a website and save the HTML files to a Google Cloud Bucket:\n",
        "\n",
        "**1. Screaming Frog**\n",
        "\n",
        "[Screaming Frog](https://www.screamingfrog.co.uk/) is a desktop app that is available for both Windows and Mac. It is a popular SEO tool that can be used to crawl websites and identify technical issues, such as broken links, duplicate content, and missing meta descriptions. Screaming Frog can also be used to export the HTML files for a website to a Google Cloud Bucket.\n",
        "\n",
        "To export the HTML files for a website to a Google Cloud Bucket using Screaming Frog, follow these steps:\n",
        "\n",
        "1. Open Screaming Frog and enter the URL of the website that you want to crawl.\n",
        "2. Click the \"Spider\" button to start the crawl.\n",
        "3. Once the crawl is complete, click the \"Export\" button and select \"Google Cloud Bucket\" from the list of export options.\n",
        "4. Enter the name of your Google Cloud Bucket and the path to the directory where you want to save the HTML files.\n",
        "5. Click the \"Export\" button to start the export process.\n",
        "\n",
        "**2. HTTrack Website Copier**\n",
        "\n",
        "HTTrack Website Copier is a free and open-source cross-platform software application that recursively downloads World Wide Web sites for offline browsing. It can also be used to export the HTML files for a website to a Google Cloud Bucket.\n",
        "\n",
        "To export the HTML files for a website to a Google Cloud Bucket using HTTrack Website Copier, follow these steps:\n",
        "\n",
        "1. Open HTTrack Website Copier and enter the URL of the website that you want to crawl.\n",
        "2. Click the \"Next\" button to continue.\n",
        "3. On the \"New Project\" screen, enter a name for your project and select the \"Download website\" option.\n",
        "4. Click the \"Next\" button to continue.\n",
        "5. On the \"Download options\" screen, select the \"Download HTML files only\" option.\n",
        "6. Click the \"Next\" button to continue.\n",
        "7. On the \"Where to download\" screen, select the \"Download to a Google Cloud Bucket\" option.\n",
        "8. Click the \"Finish\" button to start the crawl and export process.\n",
        "\n",
        "**3. Scriptable Browser**\n",
        "\n",
        "Do you need a full browser, for functionality which isn't currently supported in any of the above crawlers?\n",
        "\n",
        "Take a look at this list https://github.com/dhamaniasad/HeadlessBrowsers\n",
        "\n",
        "You'll have to extract `a href={url}` from each page and create a queue to crawl your own content, but you will be able to do so on any site you can access via a browser.\n",
        "\n",
        "For easiest onboarding, check out https://www.cypress.io/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLuJp2_-HiF1"
      },
      "source": [
        "### Prerequesites\n",
        "\n",
        "* A Google Cloud Account\n",
        "* A Google Cloud Project\n",
        "* A Google Cloud Bucket\n",
        "* A publicly available website without dynamically loading content\n",
        "  * if you something more sophisticated than `scrapy`, see the `alternatives` section"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JPrLf-uKLA-"
      },
      "source": [
        "### Let's do this\n",
        "\n",
        "**Process Summary**\n",
        "\n",
        "1. Authenticate to Google Cloud (must already have an account)\n",
        "1. Install Dependancies\n",
        "  * [Scrapy](http://scrapy.org) to crawl websites\n",
        "  * [Crochet](https://pypi.org/project/crochet/) to execute the crawler in a non-blocking thread\n",
        "1. Configure Scrapy Crawler (enter your URL & Bucket)\n",
        "1. Execute the Crawler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "-20gbO_P5z-B"
      },
      "outputs": [],
      "source": [
        "# @title Authenticate to Google Cloud (must already have an account)\n",
        "from google.colab import auth as google_auth\n",
        "google_auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "35zxH_ddG1Kl"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "D1y2x1PU2y5i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35a7f3e2-18cd-4815-b239-97a89db14f94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "CRITICAL:twisted:Unhandled error in Deferred:\n",
            "CRITICAL:twisted:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/twisted/internet/defer.py\", line 1092, in _runCallbacks\n",
            "    current.result = callback(  # type: ignore[misc]\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/crochet/_eventloop.py\", line 121, in put\n",
            "    err(result, \"Unhandled error in EventualResult\")\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/twisted/python/log.py\", line 124, in err\n",
            "    _stuff = failure.Failure()\n",
            "             ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/twisted/python/failure.py\", line 288, in __init__\n",
            "    raise NoCurrentExceptionError()\n",
            "twisted.python.failure.NoCurrentExceptionError\n"
          ]
        }
      ],
      "source": [
        "# @title Install Dependancies\n",
        "!pip install scrapy bs4 google.cloud crochet uuid python-slugify -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "8n1h4ejLHybT"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "xT7ls2F-3LXv"
      },
      "outputs": [],
      "source": [
        "# @title Configure Scrapy Crawler (enter your URL & Bucket)\n",
        "\n",
        "# Use the params on the right for convenience.\n",
        "website_url = 'https://help.kryptomate.com/en//index.html' # @param {type:\"string\"}\n",
        "storage_bucket = 'chatbot-bucket-km9272025' # @param {type:\"string\"}\n",
        "metadata_filename = 'kryptomate_metadata.jsonl' # @param {type:\"string\"}\n",
        "remove_url_fragment_after_hash = True # @param {type:\"boolean\"}\n",
        "remove_url_fragment_after_question = True # @param {type:\"boolean\"}\n",
        "wait_for_seconds = 6000 # @param {type:\"number\"}\n",
        "\n",
        "# What file extensions should the crawler ignore?\n",
        "disallowed_extensions = [\".pdf\", \".css\", \".txt\", \".png\", \".jpg\", \".jpeg\",\n",
        "                         \".webp\", \".gif\", \".svg\", \".ico\", \".woff\", \".woff2\",\n",
        "                         \".ttf\", \".otf\", \".eot\", \".mp3\", \".mp4\", \".m4a\",\n",
        "                         \".m4v\", \".mov\", \".webm\", \".mkv\", \".avi\", \".bit\",\n",
        "                         \".zip\", \".tar\", \".gz\", \".7z\", \".tor\", \".rar\",\n",
        "                         \".js\",\".json\", \".jsonl\", \".torrent\",\n",
        "                         \".xml\", \".xsl\", \".rss\", \".atom\"]\n",
        "\n",
        "# Import some basic, common modules.\n",
        "from collections import deque\n",
        "from google.cloud import storage\n",
        "from slugify import slugify\n",
        "import re\n",
        "import random\n",
        "import json\n",
        "import uuid\n",
        "from urllib.parse import urljoin\n",
        "\n",
        "# Ensures we don't crawl the same URL twice.\n",
        "completed_urls = []\n",
        "# Acts as a buffer, containing lines for the metadata file.\n",
        "metadata_buffer = deque()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Helper functions to manage URLs.\n",
        "def is_disallowed_extension(url):\n",
        "    \"\"\"Disallows URLs which end in any of the extensions in the `disallowed_extensions` list.\n",
        "\n",
        "    Args:\n",
        "        url (str): The URL to check.\n",
        "\n",
        "    Returns:\n",
        "        bool: True if the URL should be disallowed, False otherwise.\n",
        "    \"\"\"\n",
        "\n",
        "    # Check if the URL ends in any of the extensions in the `disallowed_extensions` list\n",
        "    for extension in disallowed_extensions:\n",
        "        if url.endswith(extension):\n",
        "            return True\n",
        "\n",
        "    # If the URL does not end in any of the extensions in the `disallowed_extensions` list, then it is allowed\n",
        "    return False\n",
        "\n",
        "\n",
        "def is_allowed_url(url):\n",
        "    \"\"\"Allows only full URLs, not a disallowed file extension, and not already crawled.\n",
        "\n",
        "    Args:\n",
        "        url (str): The URL to check.\n",
        "\n",
        "    Returns:\n",
        "        bool: True if the URL should be allowed, False otherwise.\n",
        "    \"\"\"\n",
        "    # require URL pattern\n",
        "    url_pattern = \"^https?:\\\\/\\\\/(?:www\\\\.)?[-a-zA-Z0-9@:%._\\\\+~#=]{1,256}\\\\.[a-zA-Z0-9()]{1,6}\\\\b(?:[-a-zA-Z0-9()@:%_\\\\+.~#?&\\\\/=]*)$\"\n",
        "    if not re.match(url_pattern, url):\n",
        "      return False\n",
        "    # disallow login & account pages\n",
        "    url_pattern = \".*(login|logout|signin|signup|signout|register|account).*\"\n",
        "    if re.match(url_pattern, url.lower()):\n",
        "      return False\n",
        "    # disallow file extensions\n",
        "    if is_disallowed_extension(url):\n",
        "      return False\n",
        "    # disallow repeat URLs\n",
        "    if url in completed_urls:\n",
        "      return False\n",
        "    return True\n",
        "\n",
        "def simplify_url(full_url):\n",
        "    \"\"\"Trim any trailing `?` and whitespace from the full URL.\n",
        "\n",
        "    Args:\n",
        "        full_url (str): The full URL.\n",
        "\n",
        "    Returns:\n",
        "        str: The full URL simplified for filename.\n",
        "    \"\"\"\n",
        "    if remove_url_fragment_after_hash:\n",
        "      pattern = r\"#.*$\"\n",
        "      full_url = re.sub(pattern, \"\", full_url)\n",
        "\n",
        "    if remove_url_fragment_after_question:\n",
        "      pattern = r\"\\?.*$\"\n",
        "      full_url = re.sub(pattern, \"\", full_url)\n",
        "\n",
        "    # remove empty trailing space and ? and #\n",
        "    full_url = full_url.strip()\n",
        "    full_url = full_url.strip('?#')\n",
        "    full_url = full_url.strip()\n",
        "\n",
        "    return full_url"
      ],
      "metadata": {
        "cellView": "form",
        "id": "U_d1iXmOBYzK"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Helper functions to manage log the metadata for each file to a .jsonl file.\n",
        "\n",
        "def add_metadata(new_metadata_dict):\n",
        "    \"\"\"Adds a single line of metadata to a buffer and may flush that buffer.\n",
        "\n",
        "    Args:\n",
        "        new_metadata_dict (dict): The dict of data which will be added.\n",
        "\n",
        "    Returns:\n",
        "        bool: None\n",
        "    \"\"\"\n",
        "    metadata_buffer.append(json.dumps(new_metadata_dict))\n",
        "    if len(metadata_buffer) >= 30:\n",
        "        flush_metadata_buffer()\n",
        "\n",
        "def flush_metadata_buffer():\n",
        "    \"\"\"Flushes the metadata buffer to the metadata file in Cloud Storage.\n",
        "\n",
        "    Returns:\n",
        "        bool: None\n",
        "    \"\"\"\n",
        "    bucket = storage.Client().get_bucket(storage_bucket)\n",
        "    # Read the existing JSON file into a string.\n",
        "    metadata = ''\n",
        "    if bucket.blob(metadata_filename).exists():\n",
        "      blob = bucket.blob(metadata_filename)\n",
        "      metadata = blob.download_as_string()\n",
        "      metadata = metadata.decode('utf-8')\n",
        "\n",
        "    while len(metadata_buffer) > 0:\n",
        "      metadata += metadata_buffer.popleft() + '\\n'\n",
        "\n",
        "    # Save file\n",
        "    blob = bucket.blob(metadata_filename)\n",
        "    blob.upload_from_string(metadata, content_type='text/html')\n",
        "    print(f'üëç metadata saved to {metadata_filename}')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "YUn7igdfBc6R"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Create the MySpider(scrapy.Spider) class\n",
        "\n",
        "# Website crawler package\n",
        "import scrapy\n",
        "from scrapy.crawler import CrawlerRunner\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Setup multi-threaded support via Twisted\n",
        "from crochet import setup, wait_for, TimeoutError\n",
        "setup()\n",
        "\n",
        "class MySpider(scrapy.Spider):\n",
        "    \"\"\"Setup a new spider with params from colab notebook.\n",
        "\n",
        "    See more options for the scapy package here:\n",
        "    https://docs.scrapy.org/en/latest/index.html\n",
        "\n",
        "    Args:\n",
        "        scrapy ([type]): [description]\n",
        "    \"\"\"\n",
        "    name = \"my_spider\"\n",
        "    start_urls = [website_url]\n",
        "    allowed_domains = [url.split('/')[2] for url in start_urls]\n",
        "\n",
        "    custom_settings = {\n",
        "        \"REQUEST_FINGERPRINTER_IMPLEMENTATION\": \"2.7\",\n",
        "        \"DOWNLOAD_TIMEOUT\": 60,\n",
        "    }\n",
        "\n",
        "    def get_tag_text(self, soup, tag):\n",
        "        \"\"\"Returns the text from the given BeautifulSoup object.\"\"\"\n",
        "        element = soup.find(tag)\n",
        "        return element.text if element else None\n",
        "\n",
        "    def get_meta_tag_with_name(self, soup, name):\n",
        "        \"\"\"Returns the text from the given BeautifulSoup object.\"\"\"\n",
        "        meta_tag = soup.find('meta', {'name': name})\n",
        "        return meta_tag['content'] if meta_tag else None\n",
        "\n",
        "    def parse(self, response):\n",
        "        # Extract the full URL of the webpage\n",
        "        full_url = response.url\n",
        "\n",
        "        # Create a filename we will save the HTML to\n",
        "        filename = re.sub(r'https?[/:]+', '-', full_url)\n",
        "        filename = f'{slugify(filename)}.html'\n",
        "        html_string = response.body\n",
        "\n",
        "        # Store file contents.\n",
        "        bucket = storage.Client().get_bucket(storage_bucket)\n",
        "        blob = bucket.blob(filename)\n",
        "        blob.upload_from_string(html_string, content_type='text/html')\n",
        "        print(filename)\n",
        "\n",
        "        # Store metadata about the file.\n",
        "        # https://cloud.google.com/generative-ai-app-builder/docs/prepare-data\n",
        "        # https://cloud.google.com/generative-ai-app-builder/docs/provide-schema\n",
        "        # Here, we will extract some data from the HTML and randomize more.\n",
        "        soup = BeautifulSoup(html_string, 'html.parser')\n",
        "        image_uri = image_name = None\n",
        "        element = soup.find('img')\n",
        "        if element and element['src']:\n",
        "          image_uri = urljoin(full_url, element['src'])\n",
        "          image_name = urljoin(full_url, element['alt'])\n",
        "        # Some very common metadata schemes are baked in and automatic.\n",
        "        tags = [\"mock\", \"unknown\", \"missing\", \"todo\"]\n",
        "        categories = [\"API Reference\", \"Blog\", \"Documentation\"]\n",
        "        # You can add your own tags as well.\n",
        "        novel_doc_status = [\"New\", \"Deprecated\", \"Stable\"]\n",
        "        add_metadata({\n",
        "            \"id\": str(uuid.uuid4()),\n",
        "            \"structData\": {\n",
        "                \"url\": full_url,\n",
        "                \"title\": self.get_tag_text(soup, 'title') or self.get_meta_tag_with_name(soup, 'title') or self.get_tag_text(soup, 'h1'),\n",
        "                \"keywords\": self.get_meta_tag_with_name(soup, 'keywords'),\n",
        "                \"description\": self.get_meta_tag_with_name(soup, 'description') or self.get_meta_tag_with_name(soup, 'desc'),\n",
        "                \"image\": {\n",
        "                    \"image_uri\": image_uri,\n",
        "                    \"image_name\": image_name,\n",
        "                },\n",
        "                \"tags\": random.choice(tags),\n",
        "                \"category\": random.choice(categories),\n",
        "                \"novel_doc_status\": random.choice(novel_doc_status),\n",
        "                # image, image_name, language_code, geolocation, question, answer, embedding_vector\n",
        "            },\n",
        "            \"content\": {\n",
        "                \"mimeType\": \"text/html\",\n",
        "                \"uri\": f\"gs://{storage_bucket}/{filename}\"\n",
        "        }})\n",
        "\n",
        "        # Recursivly follow hrefs on the page\n",
        "        for href in response.css(\"a::attr(href)\").getall():\n",
        "            next_url = urljoin(full_url, simplify_url(href))\n",
        "            if is_allowed_url(next_url):\n",
        "                completed_urls.append(next_url)\n",
        "                yield response.follow(next_url, callback=self.parse)\n",
        "\n",
        "\n",
        "class CrawlerManager:\n",
        "    @wait_for(float(wait_for_seconds))\n",
        "    # @wait_for(float(3))\n",
        "    def start_crawler(self):\n",
        "        \"\"\"Run spider with MySpider\"\"\"\n",
        "        self.crawler = CrawlerRunner()\n",
        "        self.d = self.crawler.crawl(MySpider)\n",
        "        return self.d\n",
        "\n",
        "    def stop_crawler(self):\n",
        "        \"\"\"Stop the crawler.\"\"\"\n",
        "        if self.crawler:\n",
        "            self.crawler.stop()\n",
        "            self.crawler = None\n",
        "        flush_metadata_buffer()\n",
        "        print(\"Spider Closed, cleanup complete.\")\n",
        "        return None\n",
        "\n"
      ],
      "metadata": {
        "id": "uGgTj-pIBg3S",
        "cellView": "form"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "QUQJXbbV2unI",
        "outputId": "ba7748b4-add6-4a7d-9bb8-208db1c6bff5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting...\n",
            "KeyboardInterrupt\n",
            "üëç metadata saved to kryptomate_metadata.jsonl\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Stopped the crawler due to KeyboardInterrupt (you clicked stop).'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "# @title Execute the Crawler\n",
        "import traceback\n",
        "def execute_the_crawler():\n",
        "    crawler_manager = CrawlerManager()\n",
        "    try:\n",
        "        print('Starting...')\n",
        "        crawler_manager.start_crawler()\n",
        "        flush_metadata_buffer()\n",
        "        return \"Done!\"\n",
        "    except KeyboardInterrupt as e:\n",
        "        print('KeyboardInterrupt')\n",
        "        flush_metadata_buffer()\n",
        "        return \"Stopped the crawler due to KeyboardInterrupt (you clicked stop).\"\n",
        "    except TimeoutError as e:\n",
        "        print('TimeoutError')\n",
        "        flush_metadata_buffer()\n",
        "        return \"Stopped the crawler due to timeout.\"\n",
        "    except Exception as e:\n",
        "        print(f\"Stopped the crawler due to an unexpected error: {type(e).__name__}\")\n",
        "        print(f\"Error details: {e}\")\n",
        "        print(\"Full traceback:\")\n",
        "        traceback.print_exc()\n",
        "        return \"Stopped the crawler due to some other error.\"\n",
        "\n",
        "\n",
        "execute_the_crawler()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "x47vqAKxG-ti"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}